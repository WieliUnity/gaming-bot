
--------------------------------------------------------------------------------
File Path: C:/Python Projects/gaming-bot\main.py
--------------------------------------------------------------------------------

# main.py
import time
import cv2
from bot.core.screen_capturer import ScreenCapturer
from bot.core.object_detector import ObjectDetector
from bot.core.target_selector import TargetSelector
from bot.config.settings import settings  # <-- Fix this line

def main():
    capturer = ScreenCapturer()
    detector = ObjectDetector()
    selector = TargetSelector()
    
    capturer.start()
    
    try:
        while True:
            frame = capturer.get_frame()
            if frame is not None:
                # Get detections from Roboflow
                detections = detector.detect(frame)
                
                # Select target using your prioritization logic
                target = selector.select_target(detections)
                
                if settings.DEBUG:   # Optional: Highlight the selected target
                    # Process and save frame (no window display)
                    _ = detector.process_frame(frame, detections,target)
            
            time.sleep(0.1)  # Reduce CPU usage
    finally:
        capturer.stop()
        #cv2.destroyAllWindows()

if __name__ == "__main__":
    main()

--------------------------------------------------------------------------------
File Path: C:/Python Projects/gaming-bot\bot\config\settings.py
--------------------------------------------------------------------------------

class Settings:
    #showing object detection overlay
    DEBUG: bool = True
    DEBUG_DIR: str = "debug_frames"  # Add this line
    
    
    # Screen capture
    MONITOR_REGION = {
        "top": 0,
        "left": 0,
        "width": 1920,
        "height": 1080
    }
    
    # Object detection
    CONFIDENCE_THRESHOLD = 0.5
    MODEL_PATH = "bot/models/tree_model_no1.onnx"
    TARGET_CLASS = "tree"  # Default target resource
    CLASS_NAMES = ["tree"]  # in settings.py
    # Controls
    CLICK_DELAY = (0.2, 0.5)  # Random delay range
    
    # Debug
    DEBUG = True  # Enable debug overlays
settings = Settings()  # <-- Add this line
    

--------------------------------------------------------------------------------
File Path: C:/Python Projects/gaming-bot\bot\config\__init__.py
--------------------------------------------------------------------------------



--------------------------------------------------------------------------------
File Path: C:/Python Projects/gaming-bot\bot\core\actions.py
--------------------------------------------------------------------------------

"""
ACTIONS MODULE
--------------
Handles mouse/keyboard actions with human-like randomization.
"""

import pyautogui
import random
import time

class Actions:
    def __init__(self):
        # Disable pyautogui's failsafe (use with caution!)
        pyautogui.FAILSAFE = False
        self.screen_width, self.screen_height = pyautogui.size()

    def human_move(self, x: int, y: int):
        """Moves mouse to (x,y) with human-like jitter/delays."""
        # Add slight offset to target
        x += random.randint(-5, 5)
        y += random.randint(-5, 5)
        
        # Non-linear movement duration
        duration = random.uniform(0.3, 0.7)
        pyautogui.moveTo(x, y, duration=duration)

    def human_click(self, x: int, y: int, button: str = 'left'):
        """Clicks at (x,y) with randomized delays."""
        self.human_move(x, y)
        time.sleep(random.uniform(0.1, 0.3))  # Pretend to "aim"
        pyautogui.click(button=button)
        time.sleep(random.uniform(0.2, 0.5))  # Post-click delay

    def press_key(self, key: str, repeats: int = 1):
        """Presses a key with human-like timing."""
        for _ in range(repeats):
            pyautogui.keyDown(key)
            time.sleep(random.uniform(0.05, 0.1))
            pyautogui.keyUp(key)
            time.sleep(random.uniform(0.1, 0.3))

--------------------------------------------------------------------------------
File Path: C:/Python Projects/gaming-bot\bot\core\object_detector.py
--------------------------------------------------------------------------------

# bot/core/object_detector.py
import cv2
import os
import numpy as np
from datetime import datetime
from bot.config.settings import settings

class ObjectDetector:
    def __init__(self):
        self.debug_dir = "debug_frames"
        os.makedirs(self.debug_dir, exist_ok=True)
        
        # Load ONNX model (exported with nms=False)
        self.net = cv2.dnn.readNetFromONNX(settings.MODEL_PATH)
        
        # YOLOv8 default dimensions
        self.input_size = 640
        
        # Confidence threshold from your settings
        self.conf_threshold = settings.CONFIDENCE_THRESHOLD
        # Optional NMS IoU threshold
        self.iou_threshold = 0.45

    def detect(self, frame):
        """Run inference using ONNX model."""
        # 1) Preprocess (direct resize to 640x640 + create blob)
        blob, (ratio_w, ratio_h) = self._preprocess(frame)

        # 2) Run forward pass
        self.net.setInput(blob)
        layer_names = self.net.getUnconnectedOutLayersNames()
        outputs = self.net.forward(layer_names)

        # 3) Postprocess
        detections = self._postprocess(outputs, ratio_w, ratio_h)
        return detections

    def _preprocess(self, frame):
        """
        Directly resize the input frame from (H,W) to (640,640).
        We then keep track of the ratio to map boxes back later.
        """
        original_h, original_w = frame.shape[:2]
        resized = cv2.resize(frame, (self.input_size, self.input_size))
        blob = cv2.dnn.blobFromImage(
            resized,
            scalefactor=1/255.0,
            size=(self.input_size, self.input_size),
            swapRB=True,
            crop=False
        )
        ratio_w = original_w / self.input_size
        ratio_h = original_h / self.input_size

        return blob, (ratio_w, ratio_h)

    # bot/core/object_detector.py
import cv2
import os
import numpy as np
from datetime import datetime
from bot.config.settings import settings

class ObjectDetector:
    def __init__(self):
        self.debug_dir = "debug_frames"
        os.makedirs(self.debug_dir, exist_ok=True)
        
        # Load ONNX model (exported with nms=False)
        self.net = cv2.dnn.readNetFromONNX(settings.MODEL_PATH)
        
        # YOLOv8 default dimensions
        self.input_size = 640
        
        # Confidence threshold from your settings
        self.conf_threshold = settings.CONFIDENCE_THRESHOLD
        # Optional NMS IoU threshold
        self.iou_threshold = 0.45

    def detect(self, frame):
        """Run inference using ONNX model."""
        # 1) Preprocess (direct resize to 640x640 + create blob)
        blob, (ratio_w, ratio_h) = self._preprocess(frame)

        # 2) Forward pass
        self.net.setInput(blob)
        layer_names = self.net.getUnconnectedOutLayersNames()
        outputs = self.net.forward(layer_names)

        # 3) Postprocess
        detections = self._postprocess(outputs, ratio_w, ratio_h)
        return detections

    def _preprocess(self, frame):
        """
        Directly resize the input frame from (H,W) to (640,640).
        We then keep track of the ratio to map boxes back later.
        """
        original_h, original_w = frame.shape[:2]
        resized = cv2.resize(frame, (self.input_size, self.input_size))
        blob = cv2.dnn.blobFromImage(
            resized,
            scalefactor=1/255.0,
            size=(self.input_size, self.input_size),
            swapRB=True,
            crop=False
        )
        ratio_w = original_w / self.input_size
        ratio_h = original_h / self.input_size

        return blob, (ratio_w, ratio_h)

    def _postprocess(self, outputs, ratio_w, ratio_h):
        """
        Convert raw YOLO outputs to detection format and run NMS in code.
        (Assumes ONNX file was exported with nms=False and single-class.)
        
        For shape (1,5,8400):
          raw[0, :, :] -> shape (5, 8400):
            channel 0 = x_center of all anchors
            channel 1 = y_center of all anchors
            channel 2 = w         of all anchors
            channel 3 = h         of all anchors
            channel 4 = confidence of all anchors
        """
        # The main output is outputs[0], shape = (1, 5, 8400)
        raw = outputs[0]

        # Remove batch dimension => shape (5, 8400)
        raw = np.squeeze(raw, axis=0)
        # debug info
        print("==== _postprocess DEBUG ====")
        print(f"raw.shape = {raw.shape} (should be (5, 8400) for single-class)")

        x_all = raw[0]  # shape (8400,)
        y_all = raw[1]
        w_all = raw[2]
        h_all = raw[3]
        conf_all = raw[4]

        bboxes = []
        confidences = []

        # 1) Parse each anchor
        for i in range(x_all.shape[0]):
            x_center = x_all[i]
            y_center = y_all[i]
            w        = w_all[i]
            h        = h_all[i]
            conf     = conf_all[i]

            # Filter by confidence
            if conf < self.conf_threshold:
                continue

            # If coords look normalized (< ~20?), multiply by 640 if needed
            # But typically these are direct pixel coords if your training used no 'end2end' export.
            # If they're obviously <1, multiply by 640:
            if x_center < 1.5 and w < 1.5:
                x_center *= self.input_size
                y_center *= self.input_size
                w        *= self.input_size
                h        *= self.input_size

            # Convert to [left, top, width, height]
            left = x_center - (w / 2)
            top  = y_center - (h / 2)

            bboxes.append([left, top, w, h])
            confidences.append(float(conf))

        print(f"Total anchors passing conf>{self.conf_threshold}: {len(bboxes)}")

        # 2) NMS
        indices = cv2.dnn.NMSBoxes(bboxes, confidences, self.conf_threshold, self.iou_threshold)
        detections = []

        for idx in indices.flatten():
            x, y, w, h = bboxes[idx]
            conf = confidences[idx]

            # Scale back to original resolution
            x *= ratio_w
            y *= ratio_h
            w *= ratio_w
            h *= ratio_h

            left   = int(x)
            top    = int(y)
            right  = int(x + w)
            bottom = int(y + h)

            detections.append({
                "label": settings.TARGET_CLASS,
                "confidence": conf,
                "bbox": [left, top, right, bottom]
            })

        print(f"Final detections after NMS: {len(detections)}")
        for det in detections[:5]:
            print("  ", det)

        print("==== End _postprocess DEBUG ====\n")
        return detections

    def process_frame(self, frame, detections, target=None):
        if settings.DEBUG:
            processed_frame = self._draw_detections(frame, detections, target)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
            cv2.imwrite(f"{self.debug_dir}/frame_{timestamp}.png", processed_frame)
        return frame

    def _draw_detections(self, frame, detections, target=None):
        overlay_frame = frame.copy()
        for detection in detections:
            x1, y1, x2, y2 = detection['bbox']
            cv2.rectangle(overlay_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(
                overlay_frame, 
                f"{detection['label']} {detection['confidence']:.2f}",
                (x1, y1 - 10), 
                cv2.FONT_HERSHEY_SIMPLEX, 
                0.5,
                (0, 255, 0), 
                2
            )

        if target:
            x1, y1, x2, y2 = target['bbox']
            cv2.rectangle(overlay_frame, (x1, y1), (x2, y2), (0, 0, 255), 3)

        return cv2.addWeighted(overlay_frame, 0.7, frame, 0.3, 0)


    def process_frame(self, frame, detections, target=None):
        """Optional: Draw detections for debug purposes."""
        if settings.DEBUG:
            processed_frame = self._draw_detections(frame, detections, target)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
            cv2.imwrite(f"{self.debug_dir}/frame_{timestamp}.png", processed_frame)
        return frame

    def _draw_detections(self, frame, detections, target=None):
        """Draw bounding boxes and optionally highlight the locked-on target."""
        overlay_frame = frame.copy()
        for detection in detections:
            x1, y1, x2, y2 = detection['bbox']
            cv2.rectangle(overlay_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(
                overlay_frame, 
                f"{detection['label']} {detection['confidence']:.2f}",
                (x1, y1 - 10), 
                cv2.FONT_HERSHEY_SIMPLEX, 
                0.5,
                (0, 255, 0), 
                2
            )

        if target:
            x1, y1, x2, y2 = target['bbox']
            cv2.rectangle(overlay_frame, (x1, y1), (x2, y2), (0, 0, 255), 3)

        return cv2.addWeighted(overlay_frame, 0.7, frame, 0.3, 0)


--------------------------------------------------------------------------------
File Path: C:/Python Projects/gaming-bot\bot\core\object_detector_Deepfake.py
--------------------------------------------------------------------------------

# bot/core/object_detector.py
import cv2
import os
import numpy as np
from datetime import datetime
from bot.config.settings import settings

class ObjectDetector:
    def __init__(self):
        self.debug_dir = "debug_frames"
        os.makedirs(self.debug_dir, exist_ok=True)
        
        # Load ONNX model (exported with nms=False)
        self.net = cv2.dnn.readNetFromONNX(settings.MODEL_PATH)
        
        # YOLOv8 default dimensions
        self.input_size = 640
        
        # Confidence threshold from your settings
        self.conf_threshold = settings.CONFIDENCE_THRESHOLD
        # Optional NMS IoU threshold
        self.iou_threshold = 0.45

    def detect(self, frame):
        """Run inference using ONNX model."""
        # 1) Preprocess (direct resize to 640x640 + create blob)
        blob, (ratio_w, ratio_h) = self._preprocess(frame)

        # 2) Run forward pass
        self.net.setInput(blob)
        layer_names = self.net.getUnconnectedOutLayersNames()
        outputs = self.net.forward(layer_names)

        # 3) Postprocess
        detections = self._postprocess(outputs, ratio_w, ratio_h)
        return detections

    def _preprocess(self, frame):
        """
        Directly resize the input frame from (H,W) to (640,640).
        We then keep track of the ratio to map boxes back later.
        """
        original_h, original_w = frame.shape[:2]
        resized = cv2.resize(frame, (self.input_size, self.input_size))
        blob = cv2.dnn.blobFromImage(
            resized,
            scalefactor=1/255.0,
            size=(self.input_size, self.input_size),
            swapRB=True,
            crop=False
        )
        ratio_w = original_w / self.input_size
        ratio_h = original_h / self.input_size

        return blob, (ratio_w, ratio_h)

    def _postprocess(self, outputs, ratio_w, ratio_h):
        """
        Convert raw YOLO outputs to detection format and run NMS in code.
        """
        # The output shape is (1, 84, 8400) for YOLOv8
        raw = np.squeeze(outputs[0], axis=0)  # Removes batch dimension: (84, 8400)
        raw = raw.T  # Transpose to get (8400, 84) where each row is a detection

        bboxes = []
        confidences = []
        class_ids = []

        for row in raw:
            # Denormalize coordinates: multiply by input_size to get pixel values
            x_center = row[0] * self.input_size
            y_center = row[1] * self.input_size
            w = row[2] * self.input_size
            h = row[3] * self.input_size

            class_scores = row[4:]
            class_id = np.argmax(class_scores)
            confidence = class_scores[class_id]

            if confidence < self.conf_threshold:
                continue

            # Convert xywh to [left, top, width, height] in 640x640 space
            left = x_center - (w / 2)
            top = y_center - (h / 2)

            bboxes.append([left, top, w, h])
            confidences.append(float(confidence))
            class_ids.append(class_id)

        # Apply NMS
        indices = cv2.dnn.NMSBoxes(
            bboxes,
            confidences,
            self.conf_threshold,
            self.iou_threshold
        )

        detections = []
        for i in indices.flatten():
            x, y, w, h = bboxes[i]
            # Scale back to original resolution
            x *= ratio_w
            y *= ratio_h
            w *= ratio_w
            h *= ratio_h

            left   = int(x)
            top    = int(y)
            right  = int(x + w)
            bottom = int(y + h)

            detections.append({
                "label": settings.CLASS_NAMES[class_ids[i]],
                "confidence": confidences[i],
                "bbox": [left, top, right, bottom]
            })

        return detections

    def process_frame(self, frame, detections, target=None):
        """Optional: Draw detections for debug purposes."""
        if settings.DEBUG:
            processed_frame = self._draw_detections(frame, detections, target)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
            cv2.imwrite(f"{self.debug_dir}/frame_{timestamp}.png", processed_frame)
        return frame

    def _draw_detections(self, frame, detections, target=None):
        """Draw bounding boxes and optionally highlight the locked-on target."""
        overlay_frame = frame.copy()
        for detection in detections:
            x1, y1, x2, y2 = detection['bbox']
            cv2.rectangle(overlay_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(
                overlay_frame, 
                f"{detection['label']} {detection['confidence']:.2f}",
                (x1, y1 - 10), 
                cv2.FONT_HERSHEY_SIMPLEX, 
                0.5,
                (0, 255, 0), 
                2
            )

        if target:
            x1, y1, x2, y2 = target['bbox']
            cv2.rectangle(overlay_frame, (x1, y1), (x2, y2), (0, 0, 255), 3)

        return cv2.addWeighted(overlay_frame, 0.7, frame, 0.3, 0)


--------------------------------------------------------------------------------
File Path: C:/Python Projects/gaming-bot\bot\core\object_detector_Org.py
--------------------------------------------------------------------------------

# bot/core/object_detector.py
import cv2
import os
import numpy as np
from datetime import datetime
from bot.config.settings import settings

class ObjectDetector:
    def __init__(self):
        self.debug_dir = "debug_frames"
        os.makedirs(self.debug_dir, exist_ok=True)
        
        # Load ONNX model (exported with nms=False)
        self.net = cv2.dnn.readNetFromONNX(settings.MODEL_PATH)
        
        # YOLOv8 default dimensions
        self.input_size = 640
        
        # Confidence threshold from your settings
        self.conf_threshold = settings.CONFIDENCE_THRESHOLD
        # Optional NMS IoU threshold
        self.iou_threshold = 0.45

    def detect(self, frame):
        """Run inference using ONNX model."""
        # 1) Preprocess (direct resize to 640x640 + create blob)
        blob, (ratio_w, ratio_h) = self._preprocess(frame)

        # 2) Run forward pass
        self.net.setInput(blob)
        layer_names = self.net.getUnconnectedOutLayersNames()
        outputs = self.net.forward(layer_names)

        # 3) Postprocess
        detections = self._postprocess(outputs, ratio_w, ratio_h)
        return detections

    def _preprocess(self, frame):
        """
        Directly resize the input frame from (H,W) to (640,640).
        We then keep track of the ratio to map boxes back later.
        """
        original_h, original_w = frame.shape[:2]
        resized = cv2.resize(frame, (self.input_size, self.input_size))
        blob = cv2.dnn.blobFromImage(
            resized,
            scalefactor=1/255.0,
            size=(self.input_size, self.input_size),
            swapRB=True,
            crop=False
        )
        ratio_w = original_w / self.input_size
        ratio_h = original_h / self.input_size

        return blob, (ratio_w, ratio_h)

    def _postprocess(self, outputs, ratio_w, ratio_h):
        """
        Convert raw YOLO outputs to detection format and run NMS in code.
        (This assumes the ONNX file was exported with nms=False.)
        """
        # The output shape is typically [1, N, 85] for YOLOv8
        # so let's squeeze out the first dimension, then transpose if needed.
        raw = np.squeeze(outputs[0], axis=0)  # shape: (N, 85) ideally

        # If your model outputs shape is (85, N), do raw = raw.T instead.
        # Make sure you know your model's exact output shape.

        # Separate out the bounding boxes & class predictions
        bboxes = []
        confidences = []
        class_ids = []

        for row in raw:
            x_center, y_center, w, h = row[0], row[1], row[2], row[3]
            class_scores = row[4:]
            class_id = np.argmax(class_scores)
            confidence = class_scores[class_id]

            # Filter by confidence
            if confidence < self.conf_threshold:
                continue

            # Convert xywh to [left, top, width, height] in 640x640 space
            left = x_center - (w / 2)
            top = y_center - (h / 2)

            bboxes.append([left, top, w, h])
            confidences.append(float(confidence))
            class_ids.append(class_id)

        # Now apply NMS in code
        indices = cv2.dnn.NMSBoxes(
            bboxes,
            confidences,
            self.conf_threshold,
            self.iou_threshold
        )

        detections = []
        for i in indices.flatten():
            x, y, w, h = bboxes[i]
            conf = confidences[i]
            # Scale back to original resolution
            x *= ratio_w
            y *= ratio_h
            w *= ratio_w
            h *= ratio_h

            left   = int(x)
            top    = int(y)
            right  = int(x + w)
            bottom = int(y + h)

            # You can choose the label based on class_ids[i] if you have multiple classes
            detections.append({
                "label": settings.TARGET_CLASS,  # or handle multiple if desired
                "confidence": conf,
                "bbox": [left, top, right, bottom]
            })

        return detections

    def process_frame(self, frame, detections, target=None):
        """Optional: Draw detections for debug purposes."""
        if settings.DEBUG:
            processed_frame = self._draw_detections(frame, detections, target)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
            cv2.imwrite(f"{self.debug_dir}/frame_{timestamp}.png", processed_frame)
        return frame

    def _draw_detections(self, frame, detections, target=None):
        """Draw bounding boxes and optionally highlight the locked-on target."""
        overlay_frame = frame.copy()
        for detection in detections:
            x1, y1, x2, y2 = detection['bbox']
            cv2.rectangle(overlay_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(
                overlay_frame, 
                f"{detection['label']} {detection['confidence']:.2f}",
                (x1, y1 - 10), 
                cv2.FONT_HERSHEY_SIMPLEX, 
                0.5,
                (0, 255, 0), 
                2
            )

        if target:
            x1, y1, x2, y2 = target['bbox']
            cv2.rectangle(overlay_frame, (x1, y1), (x2, y2), (0, 0, 255), 3)

        return cv2.addWeighted(overlay_frame, 0.7, frame, 0.3, 0)


--------------------------------------------------------------------------------
File Path: C:/Python Projects/gaming-bot\bot\core\screen_capturer.py
--------------------------------------------------------------------------------

"""
SCREEN CAPTURER MODULE
-----------------------
Handles screen capture using MSS library.
Captures frames in BGR format for OpenCV compatibility.
"""

import mss
import threading
import numpy as np
from bot.config.settings import settings  # <-- Import settings

class ScreenCapturer:
    def __init__(self):
        self.monitor = settings.MONITOR_REGION
        self.latest_frame = None
        self.running = False
        self.thread = None

    def start(self):
        self.running = True
        self.thread = threading.Thread(target=self._update_loop, daemon=True)
        self.thread.start()

    def _update_loop(self):
        with mss.mss() as sct:
            while self.running:
                # Grab the screen (BGRA by default)
                raw = sct.grab(self.monitor)
                # Convert to a NumPy array and discard alpha channel
                frame_bgra = np.array(raw)  # shape: (H, W, 4)
                frame_bgr = frame_bgra[:, :, :3]  # keep only B, G, R
                self.latest_frame = frame_bgr

    def get_frame(self):
        # Return a copy so we don't accidentally modify the live frame
        return self.latest_frame.copy() if self.latest_frame is not None else None

    def stop(self):
        self.running = False
        if self.thread:
            self.thread.join()


--------------------------------------------------------------------------------
File Path: C:/Python Projects/gaming-bot\bot\core\target_selector.py
--------------------------------------------------------------------------------

# bot/core/target_selector.py
import time

class TargetSelector:
    def __init__(self):
        self.current_target = None
        self.target_lock_duration = 5  # Seconds to keep target if lost
        self.last_target_time = 0

    def select_target(self, detections):
        if not detections:
            return None

        # If we have a current target that's still visible, keep it
        if self.current_target:
            for detection in detections:
                if detection['label'] == self.current_target['label'] and \
                   self._boxes_overlap(detection['bbox'], self.current_target['bbox']):
                    return self.current_target

            # If target lost but within lock duration, keep it
            if (time.time() - self.last_target_time) < self.target_lock_duration:
                return self.current_target

        # Select new target based on largest bounding box area
        largest = max(detections, key=lambda x: self._bbox_area(x['bbox']))
        self.current_target = largest
        self.last_target_time = time.time()
        return largest

    def _bbox_area(self, bbox):
        x1, y1, x2, y2 = bbox
        return (x2 - x1) * (y2 - y1)

    def _boxes_overlap(self, box1, box2, threshold=0.7):
        # Calculate intersection over union (IoU)
        x1_i = max(box1[0], box2[0])
        y1_i = max(box1[1], box2[1])
        x2_i = min(box1[2], box2[2])
        y2_i = min(box1[3], box2[3])
        
        intersection = max(0, x2_i - x1_i) * max(0, y2_i - y1_i)
        area1 = self._bbox_area(box1)
        area2 = self._bbox_area(box2)
        
        iou = intersection / (area1 + area2 - intersection)
        return iou > threshold

--------------------------------------------------------------------------------
File Path: C:/Python Projects/gaming-bot\bot\core\__init__.py
--------------------------------------------------------------------------------



--------------------------------------------------------------------------------
File Path: C:/Python Projects/gaming-bot\scripts\combine_code.py
--------------------------------------------------------------------------------

import os

# Define the folder path and output file
folder_path = r"C:/Python Projects/gaming-bot"
output_file = "all_raw_code.txt"

# Open the output file for writing
with open(output_file, "w", encoding="utf-8") as outfile:
    for root, dirs, files in os.walk(folder_path):
        for file in files:
            if file.endswith(".py"):
                # Get the full path to the .py file
                file_path = os.path.join(root, file)
                
                # Write a separator and the file path
                outfile.write(f"\n{'-'*80}\n")
                outfile.write(f"File Path: {file_path}\n")
                outfile.write(f"{'-'*80}\n\n")
                
                # Read the .py file's content and write it to the output file
                with open(file_path, "r", encoding="utf-8") as infile:
                    outfile.write(infile.read())
                    outfile.write("\n")

print(f"All Python files have been combined into {output_file}.")


--------------------------------------------------------------------------------
File Path: C:/Python Projects/gaming-bot\scripts\dataset_capture.py
--------------------------------------------------------------------------------

# scripts/dataset_capture.py
import sys
import os

# Add project root to Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Rest of your imports
import cv2
import time
from datetime import datetime
from bot.core.screen_capturer import ScreenCapturer

def capture_training_data(
    output_dir: str = "train_data",
    interval: int = 2,          # Seconds between captures
    max_captures: int = 1000,   # Max images to collect
    region: dict = {"top": 100, "left": 0, "width": 1920, "height": 980}  # Exclude UI
):
    os.makedirs(output_dir, exist_ok=True)
    capturer = ScreenCapturer(monitor=region)
    
    print(f"Capturing {max_captures} screenshots to {output_dir}...")
    for i in range(max_captures):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        frame = capturer.capture_frame()
        cv2.imwrite(f"{output_dir}/capture_{timestamp}_{i}.jpg", frame)
        time.sleep(interval)
        print(f"Captured image {i+1}/{max_captures}")

if __name__ == "__main__":
    capture_training_data()
